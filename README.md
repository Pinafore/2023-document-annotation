# Document Annotation App

Welcome to the Document Annotation NLP Tool â€“ an NLP topic modeling tool with active learning, designed to streamline document labeling.

## Getting Started

To run the app interface locally with the default Congressional Bill dataset, follow these steps:

1. Open your terminal.
2. Navigate to the `flask_app` directory using the `cd` command.
3. Run the following command, replacing `<your port number>` with your desired port number:

```
cd flask_app
flask run -p <your port number>
```

To process your own dataset, run the following command to see the available arguments and options:

```
python data_process.py --help
```

## Dataset Information

The experiments are run on the following two datasets:

1. **20newsgroup**: A dataset of newsgroup documents.
2. **congressional_bill_project_dataset**: A dataset of Congressional Bill documents.

For the Congressional Bill dataset, the app uses data from the following sources:
- [Comparative Agendas Project](https://www.comparativeagendas.net/us)
- [Congressional Bills](http://www.congressionalbills.org)

To align topic numbers with labels for Congressional Bill dataset, refer to the [Codebook](https://comparativeagendas.s3.amazonaws.com/codebookfiles/Codebook_PAP_2019.pdf).

## Run in Colab

You can also access the theoretical experiments using Google Colab. Click the badge below to open the app notebook (download the code from github and upload it to google drive first):

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pinafore/2023-document-annotation/blob/working-app/synthetic_experiment.ipynb)

## Pipeline

### Step 1: Data Preprocessing

To preprocess your dataset for analysis, follow these steps:

1. Ensure your dataset is in JSON format, readable by [pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html).
2. Your dataset table should have the columns: `text`, `label`, and `sub_labels`.
3. Run the preprocessing script:

  ```
  cd Topic_Models
  python data_process.py 
  ```

### Step 2: Topic Model Training

Choose from a range of topic models and train them using your preprocessed data:

- LDA (Latent Dirichlet Allocation)
- Supervised LDA
- Embedded Topic Model
- Contextualized Topic Model
- Bertopic
- Partially Labeled LDA
- Labeled LDA

Run the training script with appropriate arguments:

```
cd Topic_Models

python train_save_topic_model.py --num_topics <number_of_topics> \ 
--num_iters <number_of_training_iterations> \
--model_type <LDA_or_SLDA_or_ETM_or_CTM_or_Bertopic_or_PLDA_or_LLDA> \
--load_data_path <Processed_pickle_data_path>
--num_topics <number_of_topics>
--raw_text_path <json_file_path_filtered_by_data_process.py>
```

### Step 3: Synthetic Experiment

To reproduce the synthetic experiment, first train your model using Step 1 or Step 2. Then, open `synthetic_experiment.ipynb` to run the models.

### Step 4: Plotting Results

To visualize the synthetic experiment results, navigate to `new_model_plot.ipynb`. This notebook allows you to read and plot the saved results.

### Trained Topic Models

All trained topic models are saved in the `./Topic_Models/Model/` directory. Model files are stored in pickle format and follow the naming convention: `{model_type}_{number_of_topics}.pkl`. For instance, you might find files like `LDA_20.pkl`.

## Acknowledgements

This project was built upon the work of the following repositories:

- [Embedded Topic Model Repository](https://github.com/lffloyd/embedded-topic-model/tree/main)
- [Contextualized Topic Models Repository](https://github.com/MilaNLProc/contextualized-topic-models)
- [BERTopic Repository](https://github.com/MaartenGr/BERTopic/tree/master)
- [tomotopy Repository](https://github.com/bab2min/tomotopy)

We extend our gratitude to the authors of these original repositories for their valuable contributions and inspiration.


